# -*- coding: utf-8 -*-
"""MLproblems1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11_0PgZzlgVo0uIiAQu93eElGOecJfGJm

## Problem 1: Gradient Descent
### **Given**
  **loss function:** J(theta_1,theta_2) = theta_1^2 +4theta1theta2 +theta_2^2
  
  *initial values:* theta_1 = -3, theta_2 = 3

  *learning rates:* alpha = 0.03 and alpha = 0.5

------------------------------------------
### **Calculate Gradients**

theta_1: dJ/dtheta_1 = 2theta_1 + 4theta_2

theta_2: dJ/dtheta_2 = 4theta_1 + 2theta_2

dJ/dtheta_1 = 2(-3) + 4(3) = 6

dJ/dtheta_2 = 4(-3) + 2(3) = -6

-----------------------------------------
### **Update Parameters**

theta_1* = theta_1 - alpha * (dJ/dtheta_1)

theta_2* = theta_2 - alpha * (dJ/dtheta_2)

*alpha = 0.03*

theta_1* = -3 - 0.03 * (6) = -3.18

theta_2* = 3 - 0.03 * (-6) = 3.18

*alpha = 0.5*

theta_1* = -3 - 0.5 * (6) = -6

theta_2* = 3 - 0.5 * (-6) = 6

-----------------------------------------
### **Calculate Loss Values**
initial loss: J(theta_1,theta_2) = (-3)^2 + 4(-3)(3) +(3)^2 = -18

loss (alpha = 0.03):

theta_1* = -3.18 and theta_2* = 3.18

J(theta_1*, theta_2*) = (-3.18)^2 +4(-3.18)(3,18) +(3.18)^2 = -20.10

loss (alpha = 0.5):

theta_1* = -6 and theta_2* = 6

J(theta_1*, theta_2*) = (-6)^2 +4(-6)(6) +(6)^2 = -72

-----------------------------------------
### **Compare Loss Values**

**inital loss:** J(-3,3) = -18
**loss w/ (alpha = 0.03):** J(-3.18,3.18) = -20.10
**loss w/(alpha = 0.5): ** J(-6,6) = -72

**ANSWER**

So, after one step of gradient decsent with alpha = 0.03, the new loss is about -20.10. After one step of gradient desent with alpha = 0.5, the new loss is about -72. The larger learning rate of alpha = 0.5 results in a much larger update and significantly worse loss, showing the impact of using a high learning rate.

## Problem 2: Polynomial Regression
"""

import numpy as np

# set random seed
np.random.seed(42)

# generate 10 samples with 2 relations
X = np.random.rand(10, 2)  # Shape: (10, 2)
print("feature matrix (X):\n", X)

# generate target var y w/ cubic
y = 4 * X[:, 0]**3 + 3 * X[:, 1]**3 + 2 * X[:, 0] * X[:, 1] + np.random.rand(10) * 0.1
print("\ntarget variable (y):\n", y)

# create polynomial (deg 3)
from itertools import combinations_with_replacement

def polynomial_features(X, degree=3):
    n_samples, n_features = X.shape
    poly_features = [np.ones(n_samples)]

    for d in range(1, degree + 1):
        for comb in combinations_with_replacement(range(n_features), d):
            poly_features.append(np.prod(X[:, comb], axis=1))

    return np.column_stack(poly_features)

X_poly = polynomial_features(X, degree=3)

# apply l2 reg
lambda_ = 0.1
I = np.eye(X_poly.shape[1])
I[0, 0] = 0

# weights
w = np.linalg.inv(X_poly.T @ X_poly + lambda_ * I) @ X_poly.T @ y

# learned eqation
terms = ["{:.4f}".format(w[0])]
feature_names = ["x1", "x2"]
k = 1

for d in range(1, 4):
    for comb in combinations_with_replacement(feature_names, d):
        terms.append("{:.4f} * {}".format(w[k], " * ".join(comb)))
        k += 1

print("learned plynomial regression equation:")
print("y = " + " + ".join(terms))

"""## Problem 3: L-p Regularization Probability

---


"""

# lasso
def lasso_regression(X, y, lambda_val, learning_rate=0.001, epochs=2000):
    m, n = X.shape
    w = np.zeros(n)
    b = 0  # Bias term

    for _ in range(epochs):
        y_pred = X @ w + b
        dw = (1 / m) * X.T @ (y_pred - y)
        db = np.mean(y_pred - y)

        # soft-thresh
        w = np.sign(w - learning_rate * dw) * np.maximum(0, np.abs(w - learning_rate * dw) - learning_rate * lambda_val)
        b -= learning_rate * db

    return w, b

# apply lasso on diff lambda vals
lambdas = [0.01, 0.1, 1.0]
zero_ratios = {}

for lambda_val in lambdas:
    w_opt, b_opt = lasso_regression(X_poly, y, lambda_val)
    zero_ratio = np.sum(w_opt == 0) / len(w_opt)
    zero_ratios[lambda_val] = zero_ratio

    print(f"lambda: {lambda_val}, zero coefficients ratio: {zero_ratio:.4f}")


print("\ninterpretation:")
print("As lambda increases, more coefficients become zero due to stronger L1 regularization, enforcing sparsity.")